# 书生浦语大模型实战营二期 · 课程笔记_01



[TOC]

## 相关资料链接

> 回放视频：https://www.bilibili.com/video/BV1Vx421X72D/
> 课程链接：https://github.com/InternLM/Tutorial/tree/camp2
> InternLM2技术报告：[arxiv.org/pdf/2403.17297.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2403.17297.pdf)



## 模型发展历程与展望

![image-20240330232041334](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20240330232041334.png)



- 初期的基础模型，一般专注于解决特定场景下的问题。随着技术的不断进步和研究的深入，比如自动摘要、对话系统、语义理解等，各种通用大语言模型逐渐涌现，体现在单个模型解决多种任务，并以多种模态输入和呈现。
- 随着ChatGPT、GPT-4、Sora,特别是Sora的出现，标志着生成式AI模型在多模态方面取得显著进展。不久前，吴恩达教授发博客呼吁"AI智能体工作流将在今年推动人工智能取得巨大进步，甚至可能超过下一代基础模型"。如果把大模型比作人，未来在处理更复杂的问题上，除了"模型"本身拥有多模态，避免不了"模型"与"模型"之间的紧密配合，就好比生活中每个人都是与他人互相协作，多智能体的协作将多个AI智能体联系在一起工作，分配任务并探讨最佳解决方案，潜力无限。





## 认识InternLM2大模型

![image-20240330234534850](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20240330234534850.png)



- InternLM最早于20230607发布，陆续发布了InternLM-7B和InternLM-20B模型。

- 20240117月，发布了基于InternLM2-Base强化训练而来的InternLM2和InternLM2-Chat，且每个版本都有1.8B、7B 及 20B等模型版本。 本次课程使用1.8B演示学习，模型介绍和下载可移步官网： 

  > InternLM：https://github.com/InternLM/InternLM

![image-20240330235407296](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20240330235407296.png)



## InternLM2主要亮点

![image-20240331000637493](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20240331000637493.png)



- 20万Token超长上下文
-  推理、数学、代码能力提升
- 丰富的结构化创作
- 提供可靠工具调用
- 配合代码注释，提高内生计算能力

此外，还具备实用的数据分析能力，能够识别并读取表格数据，分析数据趋势，并根据需求绘制种统计图，以及调用机器习算法进行数据分析和预测，为用户提供更深入的洞察和数据驱动的决策支持。



### 模型如何应用

![image-20240331001320437](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20240331001320437.png)



- **模型选型**: 基于模型认识，根据需求，优先选择合适的模型。 
- **业务场景分析**: 业务与算力资源的取舍。
- **参数微调**: 确定调参方式。
- **构建智能体**: 复杂业务场景下，整合多功能。
- **模型评测**: 使用OpenCompass等工具进行评测。
- **模型部署**: 最终模型部署到实际应用中。



###### 思考：续训/全参数微调和部分调参的区别？

查阅资料得知：

| 类型                                     |                             定义                             |                             特点                             |
| ---------------------------------------- | :----------------------------------------------------------: | :----------------------------------------------------------: |
| 续训（Fine-tuning）                      | 续训是在一个已经在大规模数据集上进行过训练的模型基础上，在一个新的较小数据集上继续进行训练来调整模型参数。 | 主要是在已有模型的基础上，在新数据集上进行少量训练，通常用于迁移学习或者适应新任务。 |
| 全参数微调（Full Parameter Fine-tuning） | 全参数微调是指将整个模型的所有参数都参与微调，重新对所有参数进行训练。 | 所有参数都会被重新调整，通常用于特定任务需要对所有参数进行重新优化的情况。 |
| 部分调参（Partial Tuning）               | 部分调参是指只对模型的部分参数进行微调，而不是对全部参数都重新训练。 | 仅对指定的参数进行微调，通常在已经训练好的模型中，对某些层或参数进行局部调整以适应新的任务或数据。 |



## **全链条开源开放体系**

![image-20240331100211485](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20240331100211485.png)



**数据 --> 预训练 --> 微调 --> 部署 --> 评测 -->应用**

​		通常的流程是首先使用数据进行预训练，然后在特定任务上进行微调，接着将模型部署到实际应用中。随后进行评测以确保模型的性能符合预期，并最终投入实际应用中。整个流程中，各个环节相互联系、相互支持，共同构建出适用于特定任务的高效模型，后续课程中也会分别演示每个环节，期待~



## 总结

1. 熟悉InternLM 实战营的学习方式，认识一群热爱大模型技术的同学
2. 对书生·浦语大模型和相关生态链产品工具已有初步了解
3. 尝试运行大模型Demo,对问答模型工作表现出浓厚兴趣
4. 浏览了一期优秀项目架构图和部分项目git代码







## InternLM2技术报告笔记

## Introduction

1. 使用监督微调（SFT）和基于人类反馈的强化学习（RLHF）确保模型符合人类指令和价值观
2. 构建32k数据集提升长上下文处理能力
3. 引入COOLRLHF解决偏好冲突，发布预RLHF和后RLHF阶段模型，贡献在模型性能和全面开发方法



### InternEvo

1. 框架在数千个GPU上扩展模型训练，包括data、tensor、sequence和pipeline等并行技术组合实现
2. 集成Zero Redundancy Optimizer策略提高GPU内存效率
3. 引入FlashAttention和混合精度训练以提高硬件利用率
4. InternEvo展现出强大的扩展性能，在保持相同批次大小的情况下，在1024个GPU上实现53%的模型计算量利用率。



报告后续内容，目前水平不足，不好理解其中名词和概念，强行总结也是知其然不知其所以然，等课程结束后，在回过头尝试阅读理解。

